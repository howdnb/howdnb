from simhash import Simhash
import jieba
import sys
import re
import gensim

# 获取文件内容
def get_file(path):
    with open(path,'r',encoding="UTF-8") as f:
       line = f.readline()
       str=""
       while line:
           str=str+line
           line=f.readline()
    return str

#将获取到的文本内容使用jieba库进行分词并进行过滤
def cut(str):
    str=jieba.cut(str)
    result=[]
    for flag in str:     #将文本中除英文和中文外的其它过滤掉
        if(re.match(u"[a-zA-Z0-9\u4e00-\u9fa5]",flag)):
            result.append(flag)
        else:
            pass
    return result

def calculate_similarity(text1,text2):
texts=[text1,text2]
    dictionary=gensim.corpora.Dictionary(texts)
    corpus=[dictionary.doc2bow(text) for text in texts]
    similarity = gensim.similarities.Similarity('-Similarity-index', corpus, num_features=len(dictionary))
    test_corpus_1 = dictionary.doc2bow(text1)
    cosine_sim = similarity[test_corpus_1][1]
    return cosine_sim

#从命令行中启动
if __name__=='__main__':
    #使用sys库获取命令行参数
    path1=sys.argv[1]
    path2=sys.argv[2]
    path3=sys.argv[3]

    #调用函数获取文件内容
    str1=get_file(path1)
    str2=get_file(path2)

    #调用函数过滤内容并分词
    text1=cut(str1)
    text2=cut(str2)

    #调用函数计算余弦相似度
    similarity=calculate_similarity(text1,text2)
    print(f"文章相似度为：%.2f"%similarity)

    #将结果保存在指定的输出文件中
    with open(path3,'w',encoding="UTF-8") as f:
        f.write("文章相似度为：%.2f"%similarity)
